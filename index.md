---
layout: homepage
---

## About Me

I am a student at **Yandex School of Data Analysis (YSDA)** (2024–2026) and **Ural Federal University (UrFU)** (2022–2026), majoring in Fundamental Informatics and Information Technologies. 

My primary research interests lie in **Machine Learning**, specifically **Deep Learning**, **Computer Vision**, and **NLP**. I am currently focused on **Mixture of Experts (MoE)** architectures, **Multimodal Models**, and their interpretability.

**Skills:**
- **Languages & Tools:** C#, C++, Python, PostgreSQL, Git, NumPy, PyTorch, Hydra, DVC, Wandb.
- **Research:** Reading and reproducing paper experiments (arXiv), working with distributed systems.

**Schools & Workshops:**
- **Federated Learning School (MBZUAI × YSDA):** Studied Mixture of Experts (MoE), focusing on the trade-off between objective and balance functions, and analyzing various routing mechanisms. Mentors: [Martin Takáč](https://mtakac.com/) and [Aleksandr Beznosikov](https://scholar.google.com/citations?user=hVVJR-sAAAAJ&hl=ru).
- **Vega Summer School on Financial Mathematics (Kaluga, Summer 2025):** Participated in an intensive school organized by Vega Institute, covering topics in advanced financial mathematics and stochastic analysis.

## News

- **[Aug. 2025 - Present]** Working as a **Middle DL Engineer** at **Sberbank** (Gigacode team), developing an AI coding assistant.
- **[Apr. 2025 - July 2025]** Interned as an **NLP Engineer** at **Yandex** (Alice team), working on function calling functionality.
- **[July 2024 - Sept. 2024]** Interned as a **C# Backend Engineer** at **SKB Kontur**, working on internal Kubernetes-like systems.
- **[2024]** Joined **Yandex School of Data Analysis**.

## Research & Projects

### [Research: Expert Specialization in Multimodal Diffusion Models](moe-research.html)
*Pavel Vasilev, Daniil Tikhonov. Mentor: Gleb Molodtsov.*

An in-depth investigation into replacing Feed-Forward Networks (FFN) with Mixture of Experts (MoE) in unified multimodal transformer architectures (like Show-o). We explored how models can autonomously specialize experts for different data modalities (Text vs. Image) and domains.
[**Read more details...**](moe-research.html)

### [InferD: Distributed LLM Inference Engine](inferd-project.html)
*Engineering Project*

Developed a decentralized engine for LLM inference across multiple nodes. Implemented load balancing using distributed hash tables (DHT), inspired by **SWARM** and **Petals**.
[**Read more details...**](inferd-project.html)
