---
layout: homepage
---

## About Me

I am a student at **Yandex School of Data Analysis (YSDA)** (2024–2026) and **Ural Federal University (UrFU)** (2022–2026), majoring in Fundamental Informatics and Information Technologies. 

My primary research interests lie in **Machine Learning**, specifically **Deep Learning**, **Computer Vision**, and **NLP**. I am currently focused on **Mixture of Experts (MoE)** architectures, **Multimodal Models**, and their interpretability.

**Skills:**
- **Languages & Tools:** C#, C++, Python, PostgreSQL, Git, NumPy, PyTorch, Hydra, DVC, Wandb.
- **Research:** Reading and reproducing paper experiments (arXiv), working with distributed systems.

## News

- **[Aug. 2025 - Present]** Working as a **Middle DL Engineer** at **Sberbank** (Gigacode team), developing an AI coding assistant.
- **[Apr. 2025 - July 2025]** Interned as an **NLP Engineer** at **Yandex** (Alice team), working on function calling functionality.
- **[July 2024 - Sept. 2024]** Interned as a **C# Backend Engineer** at **SKB Kontur**, working on internal Kubernetes-like systems.
- **[2024]** Joined **Yandex School of Data Analysis**.

## Research & Projects

### [Research: Expert Specialization in Multimodal Diffusion Models](moe-research.html)
*Pavel Vasilev, Daniil Tikhonov. Mentor: Gleb Molodtsov.*

An in-depth investigation into replacing Feed-Forward Networks (FFN) with Mixture of Experts (MoE) in unified multimodal transformer architectures (like Show-o). We explored how models can autonomously specialize experts for different data modalities (Text vs. Image) and domains.
[**Read more details...**](moe-research.html)

### InferD: Distributed LLM Inference Engine
*Engineering Project*

Developed a decentralized engine for LLM inference across multiple nodes. Implemented load balancing using distributed hash tables (DHT), inspired by **SWARM** and **Petals**.
[**GitHub**](https://github.com/sellerbto/InferD/tree/defence)
